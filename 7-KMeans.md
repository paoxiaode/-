## 7-K-Means

**K-Means（K均值算法）是一种无监督的聚类算法，即将无标签的原始数据按照间距大小分为 $k$ 个类别。**



### 算法过程

设中心-----确定分类-------根据分类求新中心------循环

<img src="imag/图片37.png" style="zoom:50%;" />
<img src="imag/图片38.png" style="zoom:50%;" />

<font color='red'>K-means分为两个阶段：</font>

* E-step：固定聚类中心，将每一个点归到最近的聚类中心。
* M-step：固定每个采样点分配到的类，重新计算聚类中心。



<font color='red'>计算复杂度：</font>

给定$N$个采样点，$K$个类

E-step: $O(NK)$

M-step:$O(N)$



**收敛条件**：畸变程度
$$
E=\sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||_2^2\\
其中，\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x 即聚类中心
$$
前后两次迭代的聚类中心没有发生改变。



### k的选择

k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，**在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点**。 基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别。



### <font color='red'>并行计算</font>

采用MapReduce方法，map函数分配每个样本到最近的中心，reduce函数负责聚类中心的更新。



### K-Means的优化

K-Means算法每一次迭代都需要遍历全量的数据，一旦数据量过大，由于计算复杂度过大迭代的次数过多，会导致**收敛速度非常慢**。



#### K-Means++

<font color='red'>改变迭代起点，减少迭代次数：改进初始聚类中心的选取，离被选取的聚类中心越远越容易被选到。</font>

原始K-means算法最开始随机选取数据集中K个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心：

假设已经选取了n个初始聚类中心(0<n<K)，则在选取第n+1个聚类中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1)时同样通过随机的方法。可以说这也符合我们的直觉：聚类中心当然是互相离得越远越好。



#### Mini Batch K-Means（采样）

<font color='red'>减少数据量：对原始数据采样进行训练。</font>

`Mini Batch K-Means`算法是`K-Means`算法的一种优化变种，采用**小规模的数据子集**(每次训练使用的数据集是在训练算法的时候随机抽取的数据子集)**减少计算时间**，同时试图优化目标函数；`Mini Batch K-Means`算法可以减少`K-Means`算法的收敛时间，而且产生的结果效果只是略差于标准`K-Means`算法。

**算法步骤如下**

1. 首先抽取部分数据集，使用`K-Means`算法构建出K个聚簇点的模型;
2. 继续抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点;
3. 更新聚簇的中心点值;
4. 重复2-3步，直到中心点稳定或者达到迭代次数为止。





### Agglomerative Clustering

<font color='red'>自下而上</font>

<img src="imag/图片39.png"  />

